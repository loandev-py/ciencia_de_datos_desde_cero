{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1a0cb7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set, NamedTuple, List, Dict, Iterable, Tuple, TypeVar\n",
    "from collections import defaultdict, Counter\n",
    "from io import BytesIO\n",
    "import re\n",
    "import math\n",
    "import requests\n",
    "import tarfile\n",
    "import glob\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86e06baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text: str) -> Set[str]:\n",
    "    text = text.lower()\n",
    "    all_words = re.findall(\"[a-z0-9]+\", text)\n",
    "    return set(all_words)\n",
    "\n",
    "assert tokenize(\"Data Science is science\") == {\"data\", \"science\", \"is\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc86baab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(NamedTuple):\n",
    "    text: str\n",
    "    is_spam: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5bcf7016",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, k: float = 0.5) -> None:\n",
    "        self.k = k  # smoothing factor\n",
    "\n",
    "        self.tokens: Set[str] = set()\n",
    "        self.token_spam_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.token_ham_counts: Dict[str, int] = defaultdict(int)\n",
    "        self.spam_messages = self.ham_messages = 0\n",
    "\n",
    "    def train(self, messages: Iterable[Message]) -> None:\n",
    "        for message in messages:\n",
    "            # Increment message counts\n",
    "            if message.is_spam:\n",
    "                self.spam_messages += 1\n",
    "            else:\n",
    "                self.ham_messages += 1\n",
    "\n",
    "            # Increment word counts\n",
    "            for token in tokenize(message.text):\n",
    "                self.tokens.add(token)\n",
    "                if message.is_spam:\n",
    "                    self.token_spam_counts[token] += 1\n",
    "                else:\n",
    "                    self.token_ham_counts[token] += 1\n",
    "\n",
    "    def _probabilities(self, token: str) -> Tuple[float, float]:\n",
    "        \"\"\"returns P(token | spam) and P(token | not spam)\"\"\"\n",
    "        spam = self.token_spam_counts[token]\n",
    "        ham = self.token_ham_counts[token]\n",
    "\n",
    "        p_token_spam = (spam + self.k) / (self.spam_messages + 2 * self.k)\n",
    "        p_token_ham = (ham + self.k) / (self.ham_messages + 2 * self.k)\n",
    "\n",
    "        return p_token_spam, p_token_ham\n",
    "\n",
    "    def predict(self, text: str) -> float:\n",
    "        text_tokens = tokenize(text)\n",
    "        log_prob_if_spam = log_prob_if_ham = 0.0\n",
    "\n",
    "        # Iterate through each word in our vocabulary.\n",
    "        for token in self.tokens:\n",
    "            prob_if_spam, prob_if_ham = self._probabilities(token)\n",
    "\n",
    "            # If *token* appears in the message,\n",
    "            # add the log probability of seeing it;\n",
    "            if token in text_tokens:\n",
    "                log_prob_if_spam += math.log(prob_if_spam)\n",
    "                log_prob_if_ham += math.log(prob_if_ham)\n",
    "\n",
    "            # otherwise add the log probability of _not_ seeing it\n",
    "            # which is log(1 - probability of seeing it)\n",
    "            else:\n",
    "                log_prob_if_spam += math.log(1.0 - prob_if_spam)\n",
    "                log_prob_if_ham += math.log(1.0 - prob_if_ham)\n",
    "\n",
    "        prob_if_spam = math.exp(log_prob_if_spam)\n",
    "        prob_if_ham = math.exp(log_prob_if_ham)\n",
    "        return prob_if_spam / (prob_if_spam + prob_if_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24d789b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [Message(\"spam rules\", is_spam=True),\n",
    "            Message(\"ham rules\", is_spam=False),\n",
    "            Message(\"hello ham\", is_spam=False)]\n",
    "\n",
    "model = NaiveBayesClassifier(k=0.5)\n",
    "model.train(messages)\n",
    "\n",
    "assert model.tokens == {\"spam\", \"ham\", \"rules\", \"hello\"}\n",
    "assert model.spam_messages == 1\n",
    "assert model.ham_messages == 2\n",
    "assert model.token_spam_counts == {\"spam\": 1, \"rules\": 1}\n",
    "assert model.token_ham_counts == {\"ham\": 2, \"rules\": 1, \"hello\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "54f2e7de",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[36]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m p_if_ham = math.exp(\u001b[38;5;28msum\u001b[39m(math.log(p) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m probs_if_ham))\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Should be about 0.83\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "text = \"hello spam\"\n",
    "\n",
    "probs_if_spam = [\n",
    "    (1 + 0.5) / (1 + 2 * 0.5),      # \"spam\"  (present)\n",
    "    1 - (0 + 0.5) / (1 + 2 * 0.5),  # \"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (1 + 2 * 0.5),  # \"rules\" (not present)\n",
    "    (0 + 0.5) / (1 + 2 * 0.5)       # \"hello\" (present)\n",
    "]\n",
    "\n",
    "probs_if_ham = [\n",
    "    (0 + 0.5) / (2 + 2 * 0.5),      # \"spam\"  (present)\n",
    "    1 - (2 + 0.5) / (2 + 2 * 0.5),  # \"ham\"   (not present)\n",
    "    1 - (1 + 0.5) / (2 + 2 * 0.5),  # \"rules\" (not present)\n",
    "    (1 + 0.5) / (2 + 2 * 0.5),      # \"hello\" (present)\n",
    "]\n",
    "\n",
    "p_if_spam = math.exp(sum(math.log(p) for p in probs_if_spam))\n",
    "p_if_ham = math.exp(sum(math.log(p) for p in probs_if_ham))\n",
    "\n",
    "# Should be about 0.83\n",
    "assert model.predict(text) == p_if_spam / (p_if_spam + p_if_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1c54c6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hilke\\AppData\\Local\\Temp\\ipykernel_16124\\2074109259.py:14: DeprecationWarning: Python 3.14 will, by default, filter extracted tar archives and reject files or modify their metadata. Use the filter argument to control this behavior.\n",
      "  tf.extractall(OUTPUT_DIR)\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "FILES = [\"20021010_easy_ham.tar.bz2\", \"20021010_hard_ham.tar.bz2\", \"20021010_spam.tar.bz2\"]\n",
    "# aqui terminan los datos, en los subdirectorios /spam, /easy_ham y /hard_ham cambie esto a donde quiera los datos\n",
    "\n",
    "OUTPUT_DIR = \"spam_data\"\n",
    "\n",
    "for filename in FILES:\n",
    "    # usa request para obtener el contenido del archivo en cada url\n",
    "    content = requests.get(f\"{BASE_URL}{filename}\").content\n",
    "    # envuelve los bytes en memoria para poder usarlos cono \"archivo\"\n",
    "    fin = BytesIO(content)\n",
    "    # y extrae todos los archivos al dir de salida especificado\n",
    "    with tarfile.open(fileobj=fin, mode='r:bz2') as tf:\n",
    "        tf.extractall(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "46709137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modifique la ruta donde tenga los archivos\n",
    "path ='spam_data/*/*'\n",
    "data: List[Message] = []\n",
    "# glob.glob devuelve nombres de archivo que coinciden con la ruta comodin\n",
    "for filename in glob.glob(path):\n",
    "    is_spam = \"ham\" not in filename\n",
    "    # hay caracteres sobrantes en los emails; el errrors='ignore' los salta en lugar de mostrar una excepcion\n",
    "    with open(filename, errors='ignore') as email_file:\n",
    "        for line in email_file:\n",
    "            if line.startswith(\"Subject: \"):\n",
    "                subject = line.lstrip(\"Subject: \")\n",
    "                data.append(Message(subject, is_spam))\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "89f3e123",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = TypeVar('X')\n",
    "\n",
    "def split_data(data: List[X], prob: float) -> Tuple[List[X], List[X]]:\n",
    "    \"\"\"Copia del código que definimos anteriormente\"\"\"\n",
    "    data = data[:]\n",
    "    random.shuffle(data)\n",
    "    cut = int(len(data) * prob)\n",
    "    return data[:cut], data[cut:]\n",
    "\n",
    "random.seed(0)\n",
    "train_messages, test_messages = split_data(data, 0.75)\n",
    "model = NaiveBayesClassifier()\n",
    "model.train(train_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0654f74c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({(False, False): 673, (True, True): 87, (True, False): 39, (False, True): 26})\n"
     ]
    }
   ],
   "source": [
    "predictions = [(message, model.predict(message.text)) for message in test_messages]\n",
    "# supone que espam_probability > 0.5 corresponde a prediccion de spam y cuenbta las combinaciones de (actual is_spam, predicted id _spam)\n",
    "confusion_matrix = Counter((message.is_spam, spam_probability > 0.5) for message, spam_probability in predictions)\n",
    "print(confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40d2c954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras más indicativas de spam:  ['assistance', 'zzzz', '95', 'attn', 'money', 'clearance', 'per', 'sale', 'systemworks', 'adv']\n",
      "Palabras más indicativas de ham:  ['spambayes', 'users', 'razor', 'zzzzteana', 'sadev', 'apt', 'perl', 'ouch', 'spamassassin', 'bliss']\n"
     ]
    }
   ],
   "source": [
    "def p_spam_given_token(token: str, model: NaiveBayesClassifier) -> float:\n",
    "    # probablemente no habria que usar metodos privados, pero es por una buena causa\\\n",
    "    prob_if_spam, prob_if_ham = model._probabilities(token)\n",
    "    return prob_if_spam / (prob_if_spam + prob_if_ham)\n",
    "words = sorted(model.tokens, key=lambda t: p_spam_given_token(t, model))\n",
    "print(\"Palabras más indicativas de spam: \", words[-10:])\n",
    "print(\"Palabras más indicativas de ham: \", words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f90788c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_final_s(word):\n",
    "    return re.sub(\"s$\", \"\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16c62a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ciencia_de_datos_desde_cero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
